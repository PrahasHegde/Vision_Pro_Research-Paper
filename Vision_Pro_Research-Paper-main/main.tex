\documentclass[sigconf, review, nonacm, anonymous]{acmart}
% \documentclass[sigconf, nonacm]{acmart}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{\providecommand\BibTeX{{Bib\TeX}}}

%% Hack to allow for comments about author contribution - not used
% \newcommand{\projectcontrib}[1]{\affiliation{\country{#1}}}

%% If you need any other LaTeX packages, add them here
\usepackage{verbatim}
\usepackage[linesnumbered, boxed, resetcount]{algorithm2e}
\usepackage{microtype}
\let\Bbbk\relax
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{array}
\usepackage{array}
\usepackage{ragged2e}
\usepackage{subcaption}
\usepackage{float} 
\usepackage{caption}
\captionsetup[figure]{font=small, labelfont=bf, textfont=rm}  % bf=bold title, rm=normal text

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\renewcommand{\comment}[1]{\textcolor{red}{[#1]}}

%% For managing citations, it is recommended to use bibliography files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style, or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include support for advanced citation of software artefact from thebiblatex-software package, also separately available on CTAN.

%% print page numbers
\settopmatter{printfolios=true} 

%% end of the preamble, start of the body of the document source.
\begin{document}

%% The "title" command has an optional parameter allowing the author to define a "short title" to be used in page headers.
\title[Vision Pro Surveillance]{Stereoscopic Liveness Detection and Face Authentication for Secure Smart Locks}

%% The "author" command is used to define the authors.
%% You shall sabmit the anonymised version of your paper for review, so no need to include author names here.
% \author{First Awsomeauthor}
% \affiliation{\institution{THWS, MAI}\city{}\country{}}

% \author{Second Greatauthor}
% \affiliation{\institution{THWS, MAI}\city{}\country{}}

% \author{Third Amazingauthor}
% \affiliation{\institution{THWS, MAI}\city{}\country{}}

% %% By default, the full list of authors will be used in the page headers. 
% %% If this is too long use "shortauthors" to define a more concise list with surnames only.
% \renewcommand{\shortauthors}{Awsomeauthor, Greatauthor, Amazingauthor}


%% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
Recent advances in Artificial Intelligence (AI) and Internet of Things (IoT) technologies have led to the replacement of traditional locking mechanisms with intelligent, computer vision-based access systems. These systems grant entry exclusively when a user is recognized by the camera, thereby automating the unlocking process. However, such systems remain susceptible to spoofing attacks, including the use of photos or videos, which compromise security. \\
This paper presents a smart locking system that integrates face recognition with liveness detection to provide secure authentication and mitigate spoofing attacks. The proposed architecture employs a stereo vision framework to estimate facial depth via epipolar geometry, effectively distinguishing between actual faces and presentation attacks. Upon successful liveness verification, the face recognition module, authenticates the user. Once the identity is confirmed, the locking mechanism is disengaged. A web interface facilitates remote user management and real-time monitoring. The experimental evaluations demonstrate that the proposed solution achieves high accuracy and an ultra-low latency, validating its feasibility for real-time, robust, and secure access control.
\end{abstract}

%% Keywords: pick words that accurately describe the work being presented. Separate the keywords with commas - not used.
\keywords{computer vision, liveness detection, smart access, artificial intelligence, iot.}

\maketitle

\vspace{-1em}
\section{Introduction}
Smart surveillance systems \cite{mordor2026} play a crucial role in home security allowing authorized access. However, traditional methods such as PIN codes, RFID cards, and fingerprints are increasingly vulnerable to forgery, and tampering \cite{zainuddin2024}. While existing face recognition algorithms offer a more seamless approach, they remain susceptible to spoofing attacks using high-resolution photos or videos and may struggle in different lighting conditions \cite{bhattacharjee2018}. Furthermore, modern solutions utilize complex machine learning models that improve accuracy but require high computational resources, limiting their use on lightweight platforms like the Raspberry Pi. These advanced systems also rely on cloud infrastructure for database storage \cite{maheshwari2017}, which adds to the overall financial cost of the system.

To overcome these challenges, we propose liveness using epipolar geometry \cite{hartley2003multiple} and facial landmarks (eyes and nose) to find depth, a lightweight yet effective measure to thwart 2-D spoofing (Fig.\ref{fig:syst}). High-speed face detection and recognition are achieved using YuNet \cite{Wu2023YuNet} and SFace \cite{Zhong2021SFace}, specifically optimized for resource-constrained environments \cite{deepface2024}. An Arduino controls a solenoid lock, while a web-based dashboard provides live monitoring and remote admin approval \cite{arduinoexpert2025}. The proposed pipeline achieves a balance between security, accuracy, and performance.

\begin{figure}[t!] 
    \centering
    \Description{Block diagram showing system components: two ESP32 cameras, Raspberry Pi processing, Arduino-actuated solenoid lock, and web dashboard.}
    \includegraphics[width=0.45\textwidth]{img/bd_simple.jpeg}
    \caption{\textbf{System Block Diagram. }We show an overview of the proposed smart lock with liveness detection, face recognition, and hardware.}
    \label{fig:syst}
\end{figure}
%******************************************************************************
\section{Related Work}
This section reviews existing systems and the computer vision algorithms that underpin our proposed solution.


\subsection{Non-Biometric Smart Locks}
Vongchumyen et al. \cite{vongchumyen2017} developed a Web-based solution, while Aswini et al. \cite{aswini2021} introduced an RFID-OTP system. While these solve the lost-key problem, they fail to offer convenience. Whether using a phone or scanning a tag, the user is required to manually interact with a device, negating the hands-free benefit of home automation.

\subsection{Monocular Biometric Systems}
To improve convenience, facial recognition was adopted, but single-camera systems face a trade-off between security and speed. Cloud-based solutions by Maheshwari and Nalini \cite{maheshwari2017} offer accuracy but suffer from latency and internet dependency. Alternatively, local offline systems like Elechi et al. \cite{elechi2022} often lack the robustness to distinguish real faces from photos. To counter this, Dasare \cite{dasare2021} introduced active liveness (hand gestures), which unfortunately adds user friction. A robust system must be both passive and local.

\subsection{Stereo Vision and The Research Gap}
To achieve passive, secure liveness detection, our work leverages Stereo Vision for 3D depth extraction. Our methodology relies on Zhang's \cite{Zhang1998} standard for camera calibration to ensure geometric accuracy and Hartley and Sturm's \cite{Hartley1997} triangulation framework to reconstruct facial landmarks. Crucially, we utilize Hirschmüller's \cite{hirschmuller2005} Semi-Global Matching (SGM) algorithm. By applying epipolar geometry constraints, SGM produces dense disparity maps efficiently on embedded hardware. While recent Foundation Models like Wen et al. \cite{wen2025} offer high accuracy, they are computationally prohibitive for microcontrollers.



%*********************************************************************************
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\textwidth]{img/complx_bd.jpeg}
    \caption{\textbf{Integrated System Architecture.} System components with data flow from stereo capture to lock actuation.}
    \label{fig:sys2}
\end{figure}


%*********************************************************************************

\section{Technical Prerequisites}
\label{sec:tech_preq}
This section outlines the mathematical foundations of stereo vision, calibration, and metric learning required for the proposed system.

\subsection{Stereo Calibration \& Epipolar Geometry}
To reconstruct 3D information, we model the camera using the Pinhole Model \cite{hartley2003multiple}. First, we estimate the \textbf{Intrinsic Matrix} ($K$) using a flexible calibration technique \cite{Zhang1998}. This accounts for the focal length ($f_x, f_y$) and optical center ($c_x, c_y$) of low-cost sensors:

\begin{equation}
    K = \begin{bmatrix} 
    f_x & 0 & c_x \\ 
    0 & f_y & c_y \\ 
    0 & 0 & 1 
    \end{bmatrix}.
    \label{eq:intrinsic}
\end{equation}

For stereo consistency, we utilize the Epipolar Constraint and the triangulation framework described in \cite{Hartley1997}. Given a point projected onto the left ($\mathbf{p}_L$) and right ($\mathbf{p}_R$) image planes, their relationship is governed by the Fundamental Matrix ($\mathbf{F}$):

\begin{equation}
    \mathbf{p}_R^T \cdot \mathbf{F} \cdot \mathbf{p}_L = 0.
    \label{eq:epipolar}
\end{equation}

\subsection{Disparity \& Depth Estimation}
To compute depth, we first calculate pixel disparity ($d$) using the \textbf{Semi-Global Block Matching (SGBM)} algorithm \cite{hirschmuller2008stereo}. Unlike simple pixel matching, SGBM enforces a global smoothness (WLS) constraint to reduce noise on textureless surfaces like skin. The depth $Z$ is inversely proportional to disparity \cite{Hartley1997}:

\begin{equation}
    Z = \frac{f \cdot B}{d}.
    \label{eq:depth}
\end{equation}

Eq.\ref{eq:depth}, where $Z$ is the distance (depth) of the object from the camera, $f$ is the rectified focal length (derived from $f_x$), $B$ is the baseline distance between cameras, and $d$ is the Disparity and ($x_L - x_R$) horizontal pixel difference between the left and right image points. This relationship allows the system to distinguish between a real face (varying $Z$) and a spoof photograph (constant $Z$).



\subsection{Deep Metric Learning and Loss Function}
To enable robust identity verification, the system map the facial images into a 128-dimensional Euclidean space. It utilizes a Convolutional Neural Network (CNN), denoted as $f_\theta(\cdot)$, to extract a feature vector. For any input face $x$, the network outputs a normalized embedding $\mathbf{v}$:

\begin{equation}
\mathbf{v} = \frac{f_\theta(x)}{||f_\theta(x)||_2}, \quad \text{where } \mathbf{v} \in \mathbb{R}^{128}.
\end{equation}

This normalization projects all facial features onto a hypersphere of fixed radius $s$, ensuring that magnitude does not affect the matching process.

To ensure these embeddings are discriminative, the network is trained using the SFace loss function \cite{Zhong2021SFace}. SFace optimizes the embedding space by enforcing explicit geometric constraints via a combined loss function $\mathcal{L}_{S}$:

\begin{equation}
\mathcal{L}_{S} = \underbrace{\log \left( 1 + e^{s (\theta_{y_i} - \beta)} \right)}_{\text{Intra-class Compactness}} + \lambda \underbrace{\log \left( 1 + e^{s (\alpha - \theta_{neg})} \right)}_{\text{Inter-class Separability}}
\label{eq:sface_brief}
\end{equation}

Eq.\ref{eq:sface_brief}, the first term forces features of the same identity to stay within an angular boundary $\beta$, while the second term pushes different identities apart by a margin $\alpha$. This clustering during training enables the use of simple distance metrics for verification.


\textbf{Cosine Similarity} is a metric used to measure how similar two vectors are irrespective of their size. Mathematically, it measures the cosine of the angle $\theta$ between two vectors projected in a multi-dimensional space.

\begin{equation}
\text{Similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \cos(\theta)
\label{eq:cosine_similarity}
\end{equation}



The resulting score ranges from -1 (completely different) to 1 (exactly the same). In the context of face recognition, this metric is preferred over Euclidean distance because it focuses on the orientation of the feature vectors rather than their magnitude.

%*****************************************************************************************

\section{Methodology}


\subsection{System Overview}
The system workflow begins with stereo vision for depth perception and anti-spoofing, followed by face recognition and a control interface for hardware actuation. The system captures synchronized video streams from two cameras, applies epipolar rectification and depth estimation to verify facial liveness, and subsequently performs identity recognition using learned facial embeddings. After successful authentication, an Arduino-controlled solenoid lock is actuated to grant physical access as in Fig.\ref{fig:sys2}.

\subsection{System Design and Implementation }


\paragraph{\textbf{Epipolar Geometry}} The stereo vision module follows the calibrated stereo camera model described in Eq.\ref{eq:intrinsic}. Intrinsic and extrinsic parameters are obtained offline using a standard chessboard calibration procedure \cite{Zhang1998}. Using these parameters, epipolar rectification is applied to align corresponding scan lines, as in Eq.~\ref{eq:epipolar}. Disparity is then computed along horizontal epipolar lines using the Semi-Global Block Matching (SGBM) algorithm, producing a dense disparity map (Fig.\ref{fig:sys1b}).

\paragraph{\textbf{Depth Estimation}}
The depth (Z) of any pixel is inversely proportional to its disparity (d) (Eq.\ref{eq:depth}).
This allows reliable estimation of relative facial depth using the calibrated stereo configuration.

\paragraph{\textbf{Facial Landmark Localization.}}
Facial landmarks are detected using the YuNet \cite{Wu2023YuNet}, which identifies both bounding boxes and precise coordinates for key facial features. These landmarks are detected in the image space and mapped to corresponding coordinates in the disparity map. This mapping enables direct association between facial landmarks and their estimated depth values.

\paragraph{\textbf{Liveness Detection}}
A human face is a 3D object where the nose is closer to the camera than the eyes. Conversely, a 2D spoof (photo) is planar, meaning the depths of the nose and eyes are approximately equal. Depth values are extracted at the landmark positions corresponding to the left eye, right eye, and nose tip:

\begin{equation}
\Delta Z = \frac{Z_{\text{left\_eye}} + Z_{\text{right\_eye}}}{2} - Z_{\text{nose}},
\label{eq:livez}
\end{equation}
where $Z_{\text{left\_eye}}$, $Z_{\text{right\_eye}}$, and $Z_{\text{nose}}$ denote the estimated depths at the corresponding facial landmark. The liveness condition is satisfied if the resulting geometric protrusion exceeds a predefined threshold. The threshold $[\Delta Z_{min}, \Delta Z_{max}]$ defined on the empirical separation between planar spoofs ($\approx 0\text{m}$) and actaul facial depth profiles ($>0.020\text{m}$), as in Appendix \ref{sec:appendix}. This range effectively rejects both 2D presentation attacks and depth estimation outliers. Liveness is validated across 3 consecutive frames to filter transient noise.



\paragraph{\textbf{Face Recognition}}
The face recognition follows a 3 stage pipeline: detection, alignment, and feature extraction. The input image $I$ is first processed by YuNet \cite{Wu2023YuNet}, an efficiency-optimized convolutional neural network. For every detected face, it outputs a bounding box $B = (x, y, w, h)$ and 5 landmarks (nose, eyes and mouth corners).

We apply Affine Transformation \cite{Deng2019ArcFace} to map the detected facial landmarks to a standard template. This accounts for rotation ($\theta$), isotropic scaling ($s$), and translation ($t_x, t_y$) via the matrix $M$, Eq.\ref{eq:affine_matrix}:

\begin{equation}
M = \begin{bmatrix}
s \cos \theta & s \sin \theta & t_x \\
-s \sin \theta & s \cos \theta & t_y
\end{bmatrix}.
\label{eq:affine_matrix}
\end{equation}

The optimal matrix parameters are estimated using the Least Squares method, minimizing the error between the detected source points ($L_{src}$) and the target template ($L_{dst}$):

\begin{equation}
\min_{M} \sum_{j=1}^{5} \| L_{dst}^{(j)} - M \cdot \tilde{L}_{src}^{(j)} \|^2.
\end{equation}

Finally, the aligned image $I_{aligned}$ ($112 \times 112$ pixels) is generated by inverse transformation:

\vspace{-0.5em}
\begin{equation}
I_{aligned}(x', y') = I( M^{-1} [x', y', 1]^T ).
\end{equation}

The aligned image is passed to SFace \cite{Zhong2021SFace}, which maps the high-dimensional pixel data into a compact 128-dimensional embedding vector $\mathbf{v} \in \mathbb{R}^{128}$. These embeddings are normalized to lie on a hypersphere, to find semantic similarity.

Cosine similarity (Eq.\ref{eq:cosine_similarity}) is used to compare the extracted embedding with stored embeddings in the database. Access is granted if the similarity exceeds a predefined threshold (0.40 for SFace), ensuring that only authorized users are recognized.




\textbf{Hardware Actuation.}
The system governs transitions between liveness verification, recognition, and access states. Upon successful authentication, a signal is transmitted via serial communication to an Arduino, which actuates a solenoid lock. A cooldown interval of 5 seconds is enforced to prevent repeated triggering.



\begin{figure}[t!]
    \centering
    \Description{Confusion matrix or comparative chart showing model performance across tested configurations.}
    \includegraphics[width=0.4\textwidth]{img/model_cf.png}
    \caption{\textbf{Confusion Matrix for YuNet + SFace.}Demonstrating perfect accuracy and low error.}
    \label{fig:cf}
\end{figure}


%*********************************************************************************
\section{Experiments and Evaluation}

\subsection{Dataset}
We collect a dataset using a laptop web camera, including 200 high-quality images from three participants who perform various head movements and facial expressions during 60-second videos. For security, images are temporarily stored on a Raspberry Pi. Our evaluation uses a test dataset of 150 images. The model creates 128-dimensional vector embeddings for each training image and averages them to form a mean embedding for each identity, stored locally on the Raspberry Pi for real-time recognition and evaluation.





\subsection{Baselines}
Our examination includes traditional approaches such as the Haar Cascade Classifier \cite{voilajonas} and the Histogram of Oriented Gradients in conjunction with support vector machine (SVM) \cite{dalaltriggs}. Additionally, we have evaluated deep learning-based models, including the Dlib (OpenCV) \cite{King2009Dlib} and the Insightface (Arcface) \cite{Deng2019ArcFace}.


\subsection{Evaluation Metrics}
To evaluate the performance of our system, we utilize standard classification metrics derived from the confusion matrix (Fig. \ref{fig:cf}): True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN). We measure \textbf{Accuracy}, representing the overall correctness of the model. Furthermore, we calculate \textbf{Precision}, defined as the ratio of correctly predicted positive observations to the total predicted positives, and \textbf{Recall} (True Positive Rate), which measures the ratio of correctly predicted positive observations to the actual class. Finally, we report the \textbf{F1-Score}, the harmonic mean of Precision and Recall that balances the two metrics.




\subsection{Quantitative Analysis}
The Haar Cascade model demonstrated a very low Accuracy (0.08) and a high False Negative rate (FN 0.87). Due to its basic feature-based approach, which is highly susceptible to changes in lighting conditions and non-frontal facial orientations. Similarly, the HOG + SVM was unable to correctly identify most authorized users (Recall 0.14), likely due to the inflexibility of HOG descriptors when faced with the varied poses and facial expressions.

Among the deep learning approaches, Dlib (OpenCV) achieved improved recognition performance (Recall 0.83), but it was accompanied by a significant security concern: a high False Positive rate (FP 0.55). This indicates a lack of discriminative ability to differentiate between users and impostors. Although InsightFace demonstrated high Accuracy (0.94) and perfect Precision (1.00), it was not selected due to its computationally intensive architecture, exceeding the capabilities of the Raspberry Pi 5 for edge deployment.

Ultimately, YuNet + SFace provided the optimal solution. It achieved perfect metrics (1.00 Accuracy, 1.00 Precision, 0.00 FP), demonstrating the ideal balance of computational efficiency and high-precision security required for the final system (Table \ref{tab:metric evaluation}).


\begin{table}[t!]
\centering
\caption{Model Evaluation}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\textbf{Models} & \textbf{Recall (TP)} & \textbf{FP} & \textbf{TN} & \textbf{FN} & \textbf{Precision} & \textbf{F1-Score} & \textbf{Accuracy} \\
\midrule
Haar Cascade & 0.13 & 0.70 & 0.30 & 0.87 & 0.16 & 0.14 & 0.08 \\
HOG + SVM & 0.14 & 0.13 & 0.87 & 0.86 & 0.52 & 0.22 & 0.52 \\
Dlib (OpenCV) & 0.83 & 0.55 & 0.45 & 0.00 & 0.60 & 0.70 & 0.60 \\
Insightface\textsuperscript{*} & 0.90 & 0.00 & 1.00 & 0.10 & 1.00 & 0.95 & 0.94 \\
YuNet + SFace (Ours) & \textbf{1.00} & \textbf{0.00} & \textbf{1.00} & \textbf{0.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} \\
\bottomrule
\end{tabular}%
}
\label{tab:metric evaluation}
\begin{flushleft}
\footnotesize * indicates that the training/testing was conducted on a PC instead of the Raspberry Pi 5, due to the computational intensity.
\end{flushleft}
\end{table}




\begin{table}[t!]
\centering
\caption{Ablation Study}
\footnotesize
\begin{tabular}{lccc>{\RaggedRight}p{2.5cm}}
\toprule
\textbf{Configuration} & \textbf{Accuracy} & \textbf{FP} & \textbf{Latency} & \textbf{Comments} \\
\midrule
YuNet + LBPH & 0.85 & 0.80 & 18 ms & Good cropping cannot fix LBPH's texture limitations. \\
Haar Cascade + SFace & 0.68 & 0.05 & 14 ms & Good recognizer limited by Haar's inability to find faces. \\
Retinaface + Arcface\textsuperscript{*} & 1.0 & 0.0 & 180 ms & Accurate but impractical for real-time use. \\
YuNet + SFace (ours) & 1.0 & 0.0 & 15 ms & \textbf{Optimal}, perfect balance of speed, size, and security. \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}



\subsection{Ablation Study}
We conducted an ablation study to isolate the impact of the detector (YuNet) and the recognizer (SFace) independently (Table \ref{tab:ablation}). We tested four configurations: YuNet + LBPH, Haar Cascade + SFace, Retinaface + Arcface, and the proposed YuNet + SFace.
We first assessed the YuNet + LBPH. Despite the integration of a robust detector, system accuracy decreased to 0.45, accompanied by a high FP (0.40). These results indicates, Local Binary Pattern Histograms (LBPH) lack the necessary feature extraction depth for secure identification.

In contrast, we explored a less robust detector by pairing Haar Cascade with SFace. Although this setup achieved low latency (14 ms), the accuracy was constrained to 0.68. The system performance was hindered by the Haar Cascade detector's frequent failure to detect faces, resulting in a significant bottleneck.

The Retinaface + Arcface configuration matched our model's perfect accuracy (1.0) and FP (0.0), it suffered from a massive latency penalty, taking 180 ms per frame. This is over 10 times slower than our proposed solution, making it impractical for real-time edge applications where rapid response is critical.

The YuNet + SFace configuration emerged as the optimal architecture. It achieved perfect accuracy (1.0) and security (0.0 FP) and also maintained an ultra-low latency of 15 ms. This confirms that YuNet + SFace successfully bridges the gap between high-performance security and real-time efficiency.

% -----------------------------
% Table 
% -----------------------------



\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/mesh2.jpeg}
        \caption{\textbf{3D Reconstruction.} Facial depth reconstructed as a mesh.}
        \label{fig:sys1a}
    \end{subfigure}%
    \hspace{0.04\textwidth}%
    \begin{subfigure}{0.22\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{img/disparity_new.jpeg}
        \caption{\textbf{Disparity.} Disparity map showing depth variations.}
        \label{fig:sys1b}
    \end{subfigure}
    \caption{\textbf{Liveness cues:} 3D reconstruction and disparity visualization.}
    \label{fig:liveness_visuals}
\end{figure}






\subsection{Qualitative Analysis}

\paragraph{\textbf{3D Reconstruction}}
Fig.\ref{fig:sys1a} shows the rebuilt 3D point cloud mesh overlaid with measured facial depths. The close match between the mesh and the person's facial shape confirms that the depth measurements are accurate. The reconstruction clearly shows the shape and depth of the face. This accuracy makes sure that the depth difference ($\Delta Z$) (Eq.\ref{eq:livez}) comes from real features of the face.


\paragraph{\textbf{Disparity Mapping}}
Fig.\ref{fig:sys1b} shows the  disparity map by the Semi-Global Block Matching (SGBM) algorithm. Looking at the image, you can clearly see the difference between the person in front and the background. The heatmap uses warmer colors to show areas that are closer, and cooler colors for areas that are farther away. The depth across the face is not the same; there is a clear change from the tip of the nose out to the ears and neck.




%*******************************************************************

\section{Conclusion}
This research presents an efficient real-time face recognition system for edge computing on the Raspberry Pi 5, featuring epipolar geometry-based liveness detection and a web management dashboard. The YuNet + SFace pipeline outperformed other models, demonstrating that high-precision biometric authentication is feasible on affordable embedded hardware.

%**********************************************************************
\section{Acknowledgment}
We would like to thank the Technical University of Applied Sciences Würzburg-Schweinfurt (THWS) for providing the infrastructure, and resources necessary to carry out this project.
We also extend our heartfelt thanks to Prof. Dr. Andreas Lehrmann for his valuable guidance, continuous support, and insightful feedback.



%**********************************************************************************************
\appendix
\section{Depth Analysis and Liveness Thresholds}
\label{sec:appendix}


% -----------------------------
% Table I
% -----------------------------
\begin{table}[H]
\centering
\caption{Expected Depth Differences}
\begin{tabular}{l>{\centering\arraybackslash}p{4cm}}
\toprule
\textbf{Classification} & 
\textbf{$\Delta z$ (m)} \\
\midrule
Average Adult Face  & 0.020--0.030 \\
Average Child Face  & 0.015--0.020 \\
Photo / Flat Surface & $\approx 0.000$ \\
\bottomrule
\end{tabular}
\label{tab:depth_differences}
\end{table}


\noindent \textbf{Table~\ref{tab:depth_differences}} shows expected depth differences for real human faces and planar surfaces. These theoretical values form the basis for liveness detection.


% -----------------------------
% Table II
% -----------------------------
\begin{table}[htbp]
\caption{Liveness Decision Parameters}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Liveness Minimum ($\Delta Z_{\min}$) & 0.015 m \\
Liveness Maximum ($\Delta Z_{\max}$) & 0.080 m \\
Consensus Frames ($N_c$) & 3 \\
\bottomrule
\end{tabular}
\label{tab:liveness_params}
\end{table}

\noindent \textbf{Table~\ref{tab:liveness_params}} lists the  Liveness Decision parameters. The depth thresholds define valid liveness ranges, and the consensus frame count ensures stability across multiple frames.

% -----------------------------
% Table III
% -----------------------------
\begin{table}[htbp]
\caption{Depth Measurements and Liveness Results}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Test Scenario} & \textbf{$Z_{eyes}$ (m)} & \textbf{$Z_{nose}$ (m)} & \textbf{$\Delta Z$ (m)} & \textbf{Result} \\
\midrule
Real Human Subject & 0.430 & 0.393 & \textbf{0.037} & \textbf{PASS} \\
Printed Photo & 0.338 & 0.335 & 0.003 & FAIL \\
Mobile Phone Screen & 0.477 & 0.356 & 0.121 & FAIL \\
\bottomrule
\end{tabular}
\label{tab:results}
\end{table}

\noindent \textbf{Table~\ref{tab:results}} compares measured depth differences for real faces and spoof attacks. Human faces fall within the liveness range and pass the test, while printed photos and phone screens fail due to inconsistent depth variation. Mobile phone screens show higher depth differences than expected for a planar surface because reflections, and display curvature introduce erroneous stereo disparities, which the system interprets as depth variation, resulting in artificially inflated Z values. Despite this, the values remain outside the valid liveness range, so the spoof is correctly rejected.



\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


\end{document}